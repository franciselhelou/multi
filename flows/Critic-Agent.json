{"id":"ec231420-4745-46a1-969c-9c485405e5f8","data":{"nodes":[{"id":"TextInput-p2N2a","type":"genericNode","position":{"x":1754.589848650816,"y":226.84583189503684},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Original Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"TextInput-p2N2a"},"selected":false,"width":384,"height":288,"positionAbsolute":{"x":1754.589848650816,"y":226.84583189503684},"dragging":false},{"id":"Prompt-FpgZh","type":"genericNode","position":{"x":2313.802262996325,"y":340.7422368725649},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are an expert in the field of {Topic}. Using your expertise, review and modify the following article according to these instructions:\nVerify Accuracy: Ensure all factual information in the article aligns with accurate and reliable data. Focus on correcting any dates, names, statistics, or other precise data if discrepancies are found.\nUse Appropriate Terminology: Replace general terms with industry-specific or topic-relevant language to improve clarity and precision.\nExplain Complex Terms Briefly: Where needed, add brief etymologies or definitions for complex terms. Do not add extra details or expand beyond what is explicitly stated in the original text.\nEnhance Logical Flow: Adjust the sentence structure or flow to improve readability, ensuring the intended meaning and message remain unchanged.\n\nI have provided content under Context Input, sourced from the web. Use this content as a reference to review and modify the article. While you can use the information from Context Input, be aware that it may contain errors or inaccuracies, so do not treat it as an absolute authority. If needed, use it to inform your fact checking, but rely on logical coherence and sound judgment to structure a clear, engaging, and accurate explanation of the topic. \n\nImportant: Do not summarize or condense the content. The original meaning, details, and length of the article must be preserved.\n\nArticle Input\n{Article_Input}\n\nContext Input\n{Context_Input}\n\nOutput Format:\nModified Article: Provide the entire article with the applied changes included.\nChanges Made: List all the modifications made, including what was adjusted and why, to ensure clarity and accuracy.\n\nOutput: Ensure the response follows this exact format, with the modified article text first and a clear, organized list of changes made.","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"Topic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"Topic","display_name":"Topic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"Article_Input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"Article_Input","display_name":"Article_Input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"Context_Input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"Context_Input","display_name":"Context_Input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["Topic","Article_Input","Context_Input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Prompt-FpgZh"},"selected":false,"width":384,"height":561},{"id":"OllamaModel-EXEXl","type":"genericNode","position":{"x":2834.169167069016,"y":370.3249330862057},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"format","value":"","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata","value":{},"display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"mirostat","value":"Disabled","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_eta","value":"","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_tau","value":"","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["llama3.1:latest","mistral:latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama3.1:latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.com/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_ctx","value":"","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_gpu","value":"","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_thread","value":"","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_last_n","value":"","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_penalty","value":"","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"stop_tokens","value":"","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system","value":"","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"tags","value":"","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.2,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"tfs_z","value":"","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":"","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":false,"display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Critic Agent(1)","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["base_url","format","metadata","mirostat","mirostat_eta","mirostat_tau","model_name","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","stop_tokens","system","tags","temperature","template","tfs_z","timeout","top_k","top_p","verbose"]}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaModel-EXEXl"},"selected":false,"width":384,"height":665},{"id":"TextOutput-DJ5Qd","type":"genericNode","position":{"x":3475.48120770669,"y":640.959536743839},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Final Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"TextOutput-DJ5Qd"},"selected":false,"width":384,"height":288},{"id":"TextInput-bKcWn","type":"genericNode","position":{"x":1715.3152853490415,"y":807.943480779264},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Topic Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"TextInput-bKcWn"},"selected":true,"width":384,"height":288,"positionAbsolute":{"x":1715.3152853490415,"y":807.943480779264},"dragging":false},{"id":"TextInput-47Ke9","type":"genericNode","position":{"x":68,"y":293.750444089844},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Context_Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"TextInput-47Ke9"},"selected":false,"width":384,"height":288,"dragging":false},{"id":"OllamaModel-csRud","type":"genericNode","position":{"x":1124.9991036296408,"y":63.14520771218855},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","title_case":false,"type":"other","_input_type":"HandleInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"format","value":"","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"metadata","value":{},"display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"mirostat","value":"Disabled","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_eta","value":"","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"mirostat_tau","value":"","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"trace_as_metadata":true,"options":["llama3.1:latest","mistral:latest"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama3.1:latest","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.com/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_ctx","value":"","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_gpu","value":"","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"num_thread","value":"","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_last_n","value":"","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"repeat_penalty","value":"","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"stop_tokens","value":"","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system","value":"","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"tags","value":"","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.2,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"tfs_z","value":"","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"timeout","value":"","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_k","value":"","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"top_p","value":"","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":false,"display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"required_inputs":["input_value","stream","system_message"]},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"required_inputs":["base_url","format","metadata","mirostat","mirostat_eta","mirostat_tau","model_name","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","stop_tokens","system","tags","temperature","template","tfs_z","timeout","top_k","top_p","verbose"]}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template","output_parser"],"beta":false,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"OllamaModel-csRud"},"selected":false,"width":384,"height":665},{"id":"Prompt-bHWYU","type":"genericNode","position":{"x":608.32152462246,"y":272.27552543988895},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"I have provided content scraped from various internet sources. Your task is to create a thorough and comprehensive summary focused only on the key subject matter, such as the importance of AI in education. Ignore and remove all unrelated content, including advertisements, promotional language, university-specific details, program listings, contact numbers, and any references to the source or origin of the material. Do not include any generic information about institutions or calls to action. Only include insightful and substantive content about AI's impact, benefits, challenges, or other relevant points. The summary should be meaningful, well-organized, and cover the essential arguments and ideas without repetition or unnecessary filler. I want the output to be at least 2000 words.\n\nScraped Content:\n{Scraped_Content}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"Scraped_Content":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"Scraped_Content","display_name":"Scraped_Content","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["Scraped_Content"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"error":null,"edited":false,"metadata":{},"lf_version":"1.0.19"},"id":"Prompt-bHWYU"},"selected":false,"width":384,"height":390}],"edges":[{"source":"Prompt-FpgZh","target":"OllamaModel-EXEXl","sourceHandle":"{dataType:Prompt,id:Prompt-FpgZh,name:prompt,output_types:[Message]}","targetHandle":"{fieldName:input_value,id:OllamaModel-EXEXl,inputTypes:[Message],type:str}","id":"reactflow__edge-Prompt-FpgZh{dataType:Prompt,id:Prompt-FpgZh,name:prompt,output_types:[Message]}-OllamaModel-EXEXl{fieldName:input_value,id:OllamaModel-EXEXl,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-EXEXl","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-FpgZh","name":"prompt","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"OllamaModel-EXEXl","target":"TextOutput-DJ5Qd","sourceHandle":"{dataType:OllamaModel,id:OllamaModel-EXEXl,name:text_output,output_types:[Message]}","targetHandle":"{fieldName:input_value,id:TextOutput-DJ5Qd,inputTypes:[Message],type:str}","id":"reactflow__edge-OllamaModel-EXEXl{dataType:OllamaModel,id:OllamaModel-EXEXl,name:text_output,output_types:[Message]}-TextOutput-DJ5Qd{fieldName:input_value,id:TextOutput-DJ5Qd,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-DJ5Qd","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-EXEXl","name":"text_output","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"TextInput-bKcWn","target":"Prompt-FpgZh","sourceHandle":"{dataType:TextInput,id:TextInput-bKcWn,name:text,output_types:[Message]}","targetHandle":"{fieldName:Topic,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","id":"reactflow__edge-TextInput-bKcWn{dataType:TextInput,id:TextInput-bKcWn,name:text,output_types:[Message]}-Prompt-FpgZh{fieldName:Topic,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"Topic","id":"Prompt-FpgZh","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-bKcWn","name":"text","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"TextInput-p2N2a","target":"Prompt-FpgZh","sourceHandle":"{dataType:TextInput,id:TextInput-p2N2a,name:text,output_types:[Message]}","targetHandle":"{fieldName:Article_Input,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","id":"reactflow__edge-TextInput-p2N2a{dataType:TextInput,id:TextInput-p2N2a,name:text,output_types:[Message]}-Prompt-FpgZh{fieldName:Article_Input,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"Article_Input","id":"Prompt-FpgZh","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-p2N2a","name":"text","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"TextInput-47Ke9","target":"Prompt-bHWYU","sourceHandle":"{dataType:TextInput,id:TextInput-47Ke9,name:text,output_types:[Message]}","targetHandle":"{fieldName:Scraped_Content,id:Prompt-bHWYU,inputTypes:[Message,Text],type:str}","id":"reactflow__edge-TextInput-47Ke9{dataType:TextInput,id:TextInput-47Ke9,name:text,output_types:[Message]}-Prompt-bHWYU{fieldName:Scraped_Content,id:Prompt-bHWYU,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"Scraped_Content","id":"Prompt-bHWYU","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-47Ke9","name":"text","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"Prompt-bHWYU","target":"OllamaModel-csRud","sourceHandle":"{dataType:Prompt,id:Prompt-bHWYU,name:prompt,output_types:[Message]}","targetHandle":"{fieldName:input_value,id:OllamaModel-csRud,inputTypes:[Message],type:str}","id":"reactflow__edge-Prompt-bHWYU{dataType:Prompt,id:Prompt-bHWYU,name:prompt,output_types:[Message]}-OllamaModel-csRud{fieldName:input_value,id:OllamaModel-csRud,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-csRud","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-bHWYU","name":"prompt","output_types":["Message"]}},"selected":false,"animated":false,"className":""},{"source":"OllamaModel-csRud","sourceHandle":"{dataType:OllamaModel,id:OllamaModel-csRud,name:text_output,output_types:[Message]}","target":"Prompt-FpgZh","targetHandle":"{fieldName:Context_Input,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"Context_Input","id":"Prompt-FpgZh","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-csRud","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-csRud{dataType:OllamaModel,id:OllamaModel-csRud,name:text_output,output_types:[Message]}-Prompt-FpgZh{fieldName:Context_Input,id:Prompt-FpgZh,inputTypes:[Message,Text],type:str}","className":""}],"viewport":{"x":127.13350478526962,"y":60.52043794657078,"zoom":0.5443772846399619}},"description":"Transform Your Business with Smart Dialogues.","name":"Critic-Agent","last_tested_version":"1.0.19","endpoint_name":"critic-agent","is_component":false}